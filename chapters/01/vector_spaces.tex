\chapter{Vector Spaces}
\begin{center}
  \emph{Linear algebra is the study of linear maps on finite-dimensional vector spaces}
\end{center}
A vector space is a set wit operations of addition and scalar multiplications that satisfy natural algebraic properties.

\section{$\mathcal{R}^n$ and $\mathcal{C}^n$}
  \subsection{Complex number}
  Assume that there exists a square root of $-1$ called $i$, then:

  \begin{multicols}{2}
    \begin{itemize}
      \item A complex number is an ordered pair $(a,b)$ where $a,b\in\mathcal{R}$ written as $a+bi$.
      \item The set of all complex number is $\mathcal{C} = \{a+bi | a,b\in\mathcal{R}\}$.
      \item Addition and multiplications are defined on $\mathcal{C}$ as:
        \begin{align*}
          (a+bi) + (c+di) &= (a+c) + (b+d)i\\
          (a+bi) \cdot (c+di) &= (ac - bd) + (ad+bc)i
        \end{align*}
    \end{itemize}
  \end{multicols}

  Complex addition and multiplication have the expected properties:

  \begin{multicols}{2}
    \begin{itemize}
      \item Commutativity: $\alpha+\beta = \beta+\alpha\land \alpha\cdot\beta=\beta\cdot\alpha\forall\alpha,\beta\in\mathcal{C}$.
      \item Associativity: $(\alpha+\beta) + \lambda = \alpha + (\beta+\lambda)\land (\alpha\cdot\beta)\cdot\lambda=\alpha\cdot(\beta\cdot\lambda)\forall\alpha,\beta,\lambda\in\mathcal{C}$.
      \item Identities: $\lambda + 0 = \lambda\land 1\cdot\lambda = \lambda\forall\lambda\in\mathcal{C}$.
      \item Additive inverse: $\forall\alpha\in\mathcal{C},\exists\beta\in\mathcal{C}\Rightarrow \alpha+\beta = 0$.
      \item Multiplicative inverse: $\forall\alpha\in\mathcal{C},\exists\beta\in\mathcal{C}\Rightarrow \alpha\cdot\beta = 1$.
      \item Distributive property: $\lambda(\alpha+\beta) = \lambda\alpha + \lambda\beta\forall\alpha,\beta,\lambda\in\mathcal{C}$.
    \end{itemize}
  \end{multicols}

  Which are proved by using the same properties on the real numbers.
  Furthermore suppose $\alpha,\beta\in\mathcal{C}$, then:

  \begin{multicols}{2}
    \begin{itemize}
      \item $-\alpha$ the additive inverse of $\alpha$: $-\alpha$ is the unique complex number such that:
        $$\alpha+(-\alpha) = 0$$
      \item Subtraction is then defined as:
        $$\beta - \alpha = \beta + (-\alpha)$$
      \item For $\alpha\neq 0$, let $\frac{1}{\alpha}$ denote the multiplicative inverse of $\alpha$, a number such that:
        $$\alpha\cdot\left(\frac{1}{\alpha}\right) = 1$$
      \item For $\alpha\neq 0$ division by $\alpha$ is defined as:
        $$\frac{\beta}{\alpha} = \beta\cdot\left(\frac{1}{\alpha}\right)$$
    \end{itemize}
  \end{multicols}

  From now on definition and theorem can be proved on both real and complex numbers, adopting the notation where $\mathcal{F}$ stands for either $\mathcal{R}$ or $\mathcal{C}$.\\
  Elements of $\mathcal{F}$ are called \emph{scalars}.
  Furthermore $\forall\alpha\in\mathcal{F}\land m\in\mathcal{N}$, $\alpha^m$ is defined as:

  $$\alpha^m = \underbrace{\alpha\cdots\alpha}_{m\text{ times}}$$

  Which implies that:

  $$(\alpha^m)^n = a^{m\cdot n}\qquad\land\qquad(\alpha\beta)^m = \alpha^m\beta^m$$

  \subsection{Lists}
  Suppose $n\in\mathcal{N}$, then a list of length $n$ is an ordered collection of $n$ elements.
  Two lists are equal if and only if they have the same length and the same elements in the same order.
  They are often noted $(z_1, \dots, z_n)$.
  By definition a list has a finite length that is a non negative integer.
  A list of length $0$ is $(\ )$.
  They differ from set in the fact that order matters and repetitions are allowed.\\

  Fixing $n$ for the rest of the chapter, $\mathcal{F}^n$ is the set of all lists of length $n$ of elements of $\mathcal{F}$:

  $$\mathcal{F}^n = \{(x_1, \dots x_n) | x_k\in\mathcal{F}\forall k \in [1,n]\}$$

  Where $x_k$ is the $k^{th}$ coordinate of $(x_1, \dots x_n)$.\\

  Addition in $\mathcal{F}^n$ is defined by adding the corresponding coordinates:

  $$(x_1,\dots, x_n) + (y_1, \dots, y_n) = (x_1 + y_1, \dots, x_n+y_n)$$

  All of the same properties of addition hold.\\

  Elements of $\mathcal{F}^n$, with $n > 1$ can be denoted as vectors with $\vec{x}$ for example.
  Furthermore each of this set has a zero: $\vec{0}$.\\

  For all $\vec{x}\in\mathcal{F}^n$, the additive inverse of $\vec{x}$, $-\vec{x}$, is the vector $-\vec{x}\in\mathcal{F}^n$ such that:

  $$\vec{x} + (-\vec{x}) = \vec{0}$$

  Thus if $\vec{x} = (x_1, \dots, x_n)$, then $-\vec{x} = (-x_1, \dots, -x_n)$.\\

  Scalar multiplication in $\mathcal{F}^n$ is the product of a number $\lambda$ and a vector in $\mathcal{F}^n$.
  it is computed by multiplying each coordinate of the vector by $\lambda$:

  $$\lambda\vec{x} = \lambda(x_1,\dots, x_n) = (\lambda x_1, \dots, \lambda x_n)$$

  Where $\lambda\in\mathcal{F}$ and $\vec{x}\in\mathcal{F}^n$.
  From a geometric standpoint scalar multiplication shrinks or stretches the vector.

  \subsection{Digression on fields}
  A field is a set containing at least two distinct elements called $\vec{0}$ and $\vec{1}$, along with operations of addition and multiplication satisfying all the previously listed properties.
  The only fields in this work are $\mathcal{R}$ and $\mathcal{C}$, but many of the definition and theorems that work for them work without change in arbitrary fields.
  Except in the inner product chapters, results that have as a hypothesis that $\mathcal{F}$ is $\mathcal{C}$, the hypothesis can be replaced with the fact that $\mathcal{F}$ is an algebraically closed field: every nonconstant polynomial with coefficients in $\mathcal{F}$ has a zero.

\section{Definition of vector space}
A vector space is a set $V$ with an addition and a scalar multiplication on $V$ that satisfies the properties listed above.\\
An addition on $V$ is a function that assigns an element $\vec{u}+\vec{v}\in V$ to each pair of elements $\vec{u}, \vec{v}\in V$.\\
A scalar multiplication on a set $V$ is a function that assigns an element $\lambda\vec{v}\in V$ to each $\lambda\in\mathcal{F}\land\vec{v}\in V$.\\
The formal definition: a vector space is a set $V$ along with an addition on $V$ and a scalar multiplication on $V$ such that the following properties hold:

\begin{multicols}{2}
  \begin{itemize}
    \item\textbf{Commutativity}: $\vec{u}+\vec{v} = \vec{v}+\vec{u}\forall\vec{u},\vec{v}\in V$.
    \item\textbf{Associativity}: $(\vec{u} + \vec{v} ) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})\land (ab)\vec{v} = a(b\vec{v})\forall \vec{u}, \vec{v},\vec{w}\in V\land \forall a,b\in\mathcal{F}$.
    \item\textbf{Additive identity}: There exist an element $\vec{0}\in V$ such that $\vec{v} + \vec{0} = \vec{v}\forall\vec{v}\in V$.
    \item\textbf{Additive inverse}: For every $\vec{v}\in V$, there exists $\vec{w}\in V$ such that $\vec{v}+\vec{w} = \vec{0}$.
    \item\textbf{Multiplicative identity}: $1\vec{v} = \vec{v}\forall v\in V$.
    \item\textbf{Distributive properties}: $a(\vec{u}+\vec{v}) = a\vec{u}+a\vec{v}\land(a+b)\vec{v} = a\vec{v} + b\vec{v}\forall a,b\in\mathcal{F}\land\vec{u},\vec{v}\in V$
  \end{itemize}
\end{multicols}

Elements of a vector space are called vectors or points.
The scalar multiplication depends on $\mathcal{F}$, so $V$ is a vector space over $\mathcal{F}$, they can be real or complex vector space.\\

  \subsection{Unique additive identity}
  A vector space has a unique additive identity.
  Suppose $\vec{0}$ and $\vec{0}'$ are both additive identities for some vector space $V$, then:

  $$\vec{0}' = \vec{0}'+ \vec{0} = \vec{0} + \vec{0}' = \vec{0}$$

  Where this holds because $\vec{0}$ is an additive identity, the second from commutativity and the third because $\vec{0}'$ is an additive identity.
  Thus $\vec{0}' = \vec{0}$ proving that $V$ has one additive identity.

  \subsection{Unique additive inverse}
  Every element in a vector space has a unique additive inverse.
  Suppose $V$ is a vector space.
  Let $\vec{v}\in V$.
  Suppose $\vec{w}$ and $\vec{w}'$ are additive inverses of $\vec{v}$, then:

  $$\vec{w} = \vec{w} + \vec{0} = \vec{w} + (\vec{v} + \vec{w}') = (\vec{w} + \vec{v}) + \vec{w} ' = \vec{0} = \vec{w}' = \vec{w}'$$

  Thus $\vec{w} = \vec{w}'$ as desired.
  For the rest of the book $V$ denotes a vector space over $\mathcal{F}$.

  \subsection{The number $0$ times a vector}
  $0\vec{v} = \vec{0}\forall \vec{v}\in V$.
  For $\vec{v}\in V$:

  $$0\vec{v} = (0+0)\vec{v} = 0\vec{v} + 0\vec{v}$$

  Adding the additive inverse of $0\vec{v}$ to both sides of the equation above gives $0 = 0\vec{v}$ as desired.

  \subsection{A number times the vector $\vec{0}$}
  $a\vec{0} = \vec{0}\forall a\in\mathcal{F}$.
  For $a\in\mathcal{F}$:

  $$a\vec{0} = a(\vec{0} + \vec{0}) = a\vec{0} + a\vec{0}$$

  Adding the additive inverse of $a\vec{0}$ to both sides of the equation above gives $\vec{0} = a\vec{0}$ as desied.

  \subsection{The number $-1$ times a vector}
  $(-1)\vec{v} = -\vec{v}\forall \vec{v}\in V$.
  For $\vec{v}\in V$:

  $$\vec{v} + (-1)\vec{v} = 1\vec{v} + (-1)\vec{v} = (1 + (-1))\vec{v} = 0\vec{v} = 0$$

  This equation says that $(-1)\vec{v}$, when added to $\vec{v}$, gives $\vec{0}$, thus $(-1)\vec{v}$ is the additive inverse of $\vec{v}$ as desired.

\section{Subspaces}
A subset $U$ of $V$ is called a subspace of $V$ if $U$ is also a vector space with the same additive identity, addition, and scalar multiplication as on $V$.\\
A subset $U$ of $V$ is a subspace of $V$ if and only if $U$ satisfies the following:

\begin{multicols}{2}
  \begin{itemize}
    \item \textbf{Additive identity}: $\vec{0}\in U$.
    \item \textbf{Closed under addition}: $\vec{u},\vec{w}\in U\Rightarrow\vec{u}+\vec{w}\in U$.
    \item \textbf{Closed under scalar  multiplication}: $a\in\mathcal{F} \land \vec{u}\in U\Rightarrow a\vec{u}\in U$.
  \end{itemize}
\end{multicols}

  \subsection{Sum of subspaces}
  Suppose $V_1, \dots, V_m$ are subspaces of $V$.
  The sum of $V_1,\dots, V_m$, $V_1+\cdots+V_m$ is the set of all possible sum of elements of the subspaces:

  $$V_1+\cdots+V_m = \{ (\vec{v}_1+\cdots + \vec{v}_m) : \vec{v}_1\in V_1, \dots \vec{v}_m\in V_m\}$$

  Suppose $V_1,\dots, V_m$ are subspaces of $V$, then $V_1  + \cdots + V_m$ is the smallest subspace of $V$ containing $V_1, \dots, V_m$.\\
  $V_1+\cdots +V_m$ contains $\vec{0}$ and is closed under addition and scalar multiplication (verify).
  Then $V_1+\cdots+ V_m$ is a subspace of $V$.
  The subspaces $V_1, \dots, V_m$ are contained in $V_1+\cdots+V_m$ and every subspace of $V$ containing $V_1,\dots,V_m$ contains $V_1+\cdots+V_m$ because subspaces must contain all finite sums of their elements.
  Thus $V_1+\cdots+V_m$ is the smallest subspace of $V$ containing $V_1,\dots,V_m$

  \subsection{Direct sums}
  Suppose $V_1, \dots, V_m$ are subspaces of $V$.
  Every element of $V_1+\cdots + V_m$ can be written as:

  $$\vec{v}_1+\cdots+\vec{v}_m$$

  Where $\vec{v}_k\in V_k$.
  If each vector in the sum can be represented in the form above in only one way is a direct sum.\\
  Suppose $V_1, \dots, V_m$ are subspaces of $V$:

  \begin{multicols}{2}
    \begin{itemize}
      \item The sum $V_1+\cdots +V_m$ is called a direct sum if each element of $V_1+\cdots +V_m$ can be written in only one way as a sum $\vec{v}_1+\cdots+\vec{v}_m$, where $\vec{v}_k\in V_k$.
      \item If $V_1+\cdots +V_m$ is a direct sum then $V_1\oplus\cdots\oplus V_m$ denotes $V_1+\cdots +V_m$.
    \end{itemize}
  \end{multicols}

  Suppose $V_1,\dots,V_m$ are subspaces of $V$.
  Then $V_1+\cdots +V_m$ is a direct sum if and only if the only way to write $\vec{0}$ as a sum of $\vec{v}_1+\cdots+\vec{v}_m$, with $\vec{v}_k\in V_k$, is by taking $\vec{v}_k = \vec{0}$.\\
  Suppose $V_1+\cdots + V_m$ is a direct sum: the only way to write $\vec{0}$ is as a sum $\vec{v}_1+\cdots+\vec{v}_m$ by taking each $\vec{v}_l = \vec{0}$.
  The only way to write $\vec{0}$ as a sum $\vec{v}_1 + \cdots + \vec{v}_m$ is by taking $\vec{v}_k = \vec{0}$.
  Now suppose that the only way to write $\vec{0}$ as a sum $\vec{v}_1+\cdots+\vec{v}_m$, where each $\vec{v}_k = \vec{0}$.
  To show that $V_1+\cdots+V_m$ is a direct sum let $\vec{v} \in V_1+\cdots + V_m$:

  $$\vec{v} = \vec{v}_1 + \cdots + \vec{v}_m$$

  For some $\vec{v}_1\in V_1, \dots, \vec{v}_m\in V_m$.
  Also  let $\vec{u} \in V_1+\cdots + V_m$:

  $$\vec{v} = \vec{u}_1 + \cdots + \vec{u}_m$$

  For some $\vec{u}_1\in V_1, \dots, \vec{u}_m\in V_m$.
  Subtracting both:

  $$\vec{0} = (\vec{v}_1 - \vec{u}_1) + \cdots + (\vec{v}_m - \vec{u}_m)$$

  Because $\vec{v}_k - \vec{u}_k\in V_k\forall k\in[1,m]\Rightarrow \vec{v}_k-\vec{u}_k = \vec{0}\Rightarrow \vec{v}_k = \vec{u}_k\forall k$.\\
  Suppose $U$ and $W$ subspaces of $V$, then:

  $$U + W \text{ is a direct sum } \Leftrightarrow U\cap W = \{\vec{0}\}$$

  Suppose that $U + W$ is a direct sum.
  If $\vec{v}\in U\cap W$, then $\vec{0} = \vec{v} + (-\vec{v})$, where $\vec{v}\in U\land -\vec{v}\in W$.
  By the unique representation of $\vec{0}$ as the sum of a vector in $U$ and in $W$, $\vec{v} = \vec{0}$, thus $U\cap W = \{\vec{0}\}$, completing the proof in one direction.
  Now suppose that $U\cap W = \{\vec{0}\}$. TO prove that $U+W$ is a direct sum suppose $\vec{u}\in U\land \vec{w}\in W$:

  $$\vec{0} + \vec{u} + \vec{w}$$

  It remains to prove that $\vec{u} = \vec{w} = \vec{0}$.
  The equation above implies that $\vec{u} = -\vec{w} ]in W$, thus $\vec{u}]in U\cap W$, hence $\vec{u} = \vec{0}$, which by the equation above implies that $\vec{w} = \vec{0}$.\\
  This only in the case of two subspaces.\\
  Sum of subspaces are analogous to unions of subsets.
  Direct sums disjoint union of subsets, where disjointness is replaced with the requirement that intersection is $\{0\}$.
